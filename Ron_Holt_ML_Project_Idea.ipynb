{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0vOweEAWPFRRu1G1mCpzE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronaldsholt/ML_Vandy/blob/main/Ron_Holt_ML_Project_Idea.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vDqf6UtxISJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background**: Predicting cell lineage patterns in RNAseq measurments.\n",
        "\n",
        "**Project Description**: Using representation learning as low dimentional input to higher dimentional reduction clustering tools like TSNE or UMAP to learn cell lineage. Also evaluate against traditional methods like PCA or Logistic PCA.\n",
        "\n",
        "**Performance Metric**: Min{L2-norm/MSE}"
      ],
      "metadata": {
        "id": "6nNhFA2eIhW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic EDA"
      ],
      "metadata": {
        "id": "WZj34ucE7G8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic EDA Questions\n",
        "\n",
        "#1. How many variations are there?\n",
        "#2. What is the highest sequence variation ?\n",
        "#3. What is the highest gene-gene correlation ?\n",
        "#4. What is the class break down?"
      ],
      "metadata": {
        "id": "XsjeICquJG8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2cdb336-79de-4e57-fbda-5a759fcb59d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%md` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling and assigning GPU\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQWPyrrZ6_I7",
        "outputId": "e262498e-a878-45a3-a154-123e5facfd9e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0rKogwR7OJ4",
        "outputId": "75850604-2543-4097-9287-ce6960d136ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# import the data from drive\n",
        "path = \"/content/drive/My Drive/rna_proj/data/scaled_all_specific_exp/mat.norm.scale.2000.csv\" \n",
        "df = pd.read_csv(path)\n",
        "df = df.drop([\"Unnamed: 0\"], axis = 1) "
      ],
      "metadata": {
        "id": "j3sZ6vod7QoJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_1 = \"/content/drive/My Drive/rna_proj/data/variable.features.csv\"\n",
        "df_feat = pd.read_csv(path_1)"
      ],
      "metadata": {
        "id": "ADeyoG3p7abN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index the columns based on 2000 features \n",
        "lst = list(df_feat[\"x\"])\n",
        "lst.append(\"Variable\") #add back the RNA seq\n",
        "\n",
        "# index df to select only these columns using intersect\n",
        "df_new = df[df.columns.intersection(lst)]"
      ],
      "metadata": {
        "id": "i5MoS9o27o1O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "P7iQWsTq7oyh",
        "outputId": "7f48dbc9-c069-4476-ce51-0fe3ef3514f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Variable     Sox17     Prex2     Terf1      Mcm3   Khdrbs2  \\\n",
              "0  AAACCCAGTTCCACAA -1.393788 -0.400318  1.358433 -0.494224 -0.084449   \n",
              "1  AAACCCATCACGAACT -0.582664 -1.409803  0.177138  1.502572 -0.084449   \n",
              "2  AAACGAAAGCCGAATG -0.146528 -0.251669 -0.801586 -0.310414 -0.084449   \n",
              "3  AAACGAAGTTCAAAGA  1.102187 -0.634825  1.040665 -0.772438 -0.084449   \n",
              "4  AAACGAATCCGACGGT  0.863034  0.133187  0.329966 -0.557825 -0.084449   \n",
              "\n",
              "      Prim2      Bag2    Ptpn18    Hs6st1  ...     Nolc1     Nfkb2     Gsto1  \\\n",
              "0  0.199815 -0.490080 -0.298271 -0.493740  ...  0.447022  0.738963  1.262297   \n",
              "1  0.554011  1.077822  1.039509  0.734416  ...  0.125040 -0.603773 -0.720412   \n",
              "2  1.765205 -0.490080 -0.186529 -0.493740  ...  0.318736 -0.603773  0.582429   \n",
              "3  2.116643  0.681951 -0.487508 -0.493740  ... -0.638577 -0.603773 -0.720412   \n",
              "4 -0.616780 -0.490080  1.091608 -0.493740  ... -1.194605 -0.603773 -0.720412   \n",
              "\n",
              "       Add3      Mxi1     Dusp5   Gm16299     Pdcd4   Plekhs1     Shtn1  \n",
              "0 -0.410792  1.378466 -0.422791 -0.043317 -0.170391 -0.078911 -0.195295  \n",
              "1 -0.980596  0.055242 -0.422791 -0.043317 -0.641089 -0.078911 -0.195295  \n",
              "2 -0.028609 -0.250889  0.645672 -0.043317 -0.641089 -0.078911 -0.195295  \n",
              "3  0.131617  0.788557 -0.422791 -0.043317 -0.136620 -0.078911 -0.195295  \n",
              "4  1.233833 -0.775900  1.532813 -0.043317  1.188186 -0.078911 -0.195295  \n",
              "\n",
              "[5 rows x 2001 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a4f4e1d-1e8a-452c-a182-77f931be1f90\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Variable</th>\n",
              "      <th>Sox17</th>\n",
              "      <th>Prex2</th>\n",
              "      <th>Terf1</th>\n",
              "      <th>Mcm3</th>\n",
              "      <th>Khdrbs2</th>\n",
              "      <th>Prim2</th>\n",
              "      <th>Bag2</th>\n",
              "      <th>Ptpn18</th>\n",
              "      <th>Hs6st1</th>\n",
              "      <th>...</th>\n",
              "      <th>Nolc1</th>\n",
              "      <th>Nfkb2</th>\n",
              "      <th>Gsto1</th>\n",
              "      <th>Add3</th>\n",
              "      <th>Mxi1</th>\n",
              "      <th>Dusp5</th>\n",
              "      <th>Gm16299</th>\n",
              "      <th>Pdcd4</th>\n",
              "      <th>Plekhs1</th>\n",
              "      <th>Shtn1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAACCCAGTTCCACAA</td>\n",
              "      <td>-1.393788</td>\n",
              "      <td>-0.400318</td>\n",
              "      <td>1.358433</td>\n",
              "      <td>-0.494224</td>\n",
              "      <td>-0.084449</td>\n",
              "      <td>0.199815</td>\n",
              "      <td>-0.490080</td>\n",
              "      <td>-0.298271</td>\n",
              "      <td>-0.493740</td>\n",
              "      <td>...</td>\n",
              "      <td>0.447022</td>\n",
              "      <td>0.738963</td>\n",
              "      <td>1.262297</td>\n",
              "      <td>-0.410792</td>\n",
              "      <td>1.378466</td>\n",
              "      <td>-0.422791</td>\n",
              "      <td>-0.043317</td>\n",
              "      <td>-0.170391</td>\n",
              "      <td>-0.078911</td>\n",
              "      <td>-0.195295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAACCCATCACGAACT</td>\n",
              "      <td>-0.582664</td>\n",
              "      <td>-1.409803</td>\n",
              "      <td>0.177138</td>\n",
              "      <td>1.502572</td>\n",
              "      <td>-0.084449</td>\n",
              "      <td>0.554011</td>\n",
              "      <td>1.077822</td>\n",
              "      <td>1.039509</td>\n",
              "      <td>0.734416</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125040</td>\n",
              "      <td>-0.603773</td>\n",
              "      <td>-0.720412</td>\n",
              "      <td>-0.980596</td>\n",
              "      <td>0.055242</td>\n",
              "      <td>-0.422791</td>\n",
              "      <td>-0.043317</td>\n",
              "      <td>-0.641089</td>\n",
              "      <td>-0.078911</td>\n",
              "      <td>-0.195295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AAACGAAAGCCGAATG</td>\n",
              "      <td>-0.146528</td>\n",
              "      <td>-0.251669</td>\n",
              "      <td>-0.801586</td>\n",
              "      <td>-0.310414</td>\n",
              "      <td>-0.084449</td>\n",
              "      <td>1.765205</td>\n",
              "      <td>-0.490080</td>\n",
              "      <td>-0.186529</td>\n",
              "      <td>-0.493740</td>\n",
              "      <td>...</td>\n",
              "      <td>0.318736</td>\n",
              "      <td>-0.603773</td>\n",
              "      <td>0.582429</td>\n",
              "      <td>-0.028609</td>\n",
              "      <td>-0.250889</td>\n",
              "      <td>0.645672</td>\n",
              "      <td>-0.043317</td>\n",
              "      <td>-0.641089</td>\n",
              "      <td>-0.078911</td>\n",
              "      <td>-0.195295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AAACGAAGTTCAAAGA</td>\n",
              "      <td>1.102187</td>\n",
              "      <td>-0.634825</td>\n",
              "      <td>1.040665</td>\n",
              "      <td>-0.772438</td>\n",
              "      <td>-0.084449</td>\n",
              "      <td>2.116643</td>\n",
              "      <td>0.681951</td>\n",
              "      <td>-0.487508</td>\n",
              "      <td>-0.493740</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.638577</td>\n",
              "      <td>-0.603773</td>\n",
              "      <td>-0.720412</td>\n",
              "      <td>0.131617</td>\n",
              "      <td>0.788557</td>\n",
              "      <td>-0.422791</td>\n",
              "      <td>-0.043317</td>\n",
              "      <td>-0.136620</td>\n",
              "      <td>-0.078911</td>\n",
              "      <td>-0.195295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAACGAATCCGACGGT</td>\n",
              "      <td>0.863034</td>\n",
              "      <td>0.133187</td>\n",
              "      <td>0.329966</td>\n",
              "      <td>-0.557825</td>\n",
              "      <td>-0.084449</td>\n",
              "      <td>-0.616780</td>\n",
              "      <td>-0.490080</td>\n",
              "      <td>1.091608</td>\n",
              "      <td>-0.493740</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.194605</td>\n",
              "      <td>-0.603773</td>\n",
              "      <td>-0.720412</td>\n",
              "      <td>1.233833</td>\n",
              "      <td>-0.775900</td>\n",
              "      <td>1.532813</td>\n",
              "      <td>-0.043317</td>\n",
              "      <td>1.188186</td>\n",
              "      <td>-0.078911</td>\n",
              "      <td>-0.195295</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2001 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a4f4e1d-1e8a-452c-a182-77f931be1f90')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2a4f4e1d-1e8a-452c-a182-77f931be1f90 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2a4f4e1d-1e8a-452c-a182-77f931be1f90');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import labels, and join on Variable barcode \n",
        "# Import the 2001 cell labels\n",
        "path_3 = \"/content/drive/My Drive/rna_proj/data/cell.types.2001.csv\"\n",
        "df_labels = pd.read_csv(path_3)\n",
        "df_labels = df_labels.rename(columns={\"Unnamed: 0\": \"Variable\"})\n",
        "df_labeled = pd.merge(df_labels, df_new, on=\"Variable\")"
      ],
      "metadata": {
        "id": "mffzFPZ38UAB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = df_labeled #rename\n",
        "df_new.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "ClUI4G3-8ZBo",
        "outputId": "cc4d6663-86cc-4110-ba7a-d236308cbf83"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Variable          cell.type     Sox17     Prex2     Terf1  \\\n",
              "0  AAACCCAGTTCCACAA  cycling_capillary -1.393788 -0.400318  1.358433   \n",
              "1  AAACCCATCACGAACT           artery_1 -0.582664 -1.409803  0.177138   \n",
              "2  AAACGAAAGCCGAATG  cycling_capillary -0.146528 -0.251669 -0.801586   \n",
              "3  AAACGAAGTTCAAAGA  cycling_capillary  1.102187 -0.634825  1.040665   \n",
              "4  AAACGAATCCGACGGT           artery_2  0.863034  0.133187  0.329966   \n",
              "\n",
              "       Mcm3   Khdrbs2     Prim2      Bag2    Ptpn18  ...     Nolc1     Nfkb2  \\\n",
              "0 -0.494224 -0.084449  0.199815 -0.490080 -0.298271  ...  0.447022  0.738963   \n",
              "1  1.502572 -0.084449  0.554011  1.077822  1.039509  ...  0.125040 -0.603773   \n",
              "2 -0.310414 -0.084449  1.765205 -0.490080 -0.186529  ...  0.318736 -0.603773   \n",
              "3 -0.772438 -0.084449  2.116643  0.681951 -0.487508  ... -0.638577 -0.603773   \n",
              "4 -0.557825 -0.084449 -0.616780 -0.490080  1.091608  ... -1.194605 -0.603773   \n",
              "\n",
              "      Gsto1      Add3      Mxi1     Dusp5   Gm16299     Pdcd4   Plekhs1  \\\n",
              "0  1.262297 -0.410792  1.378466 -0.422791 -0.043317 -0.170391 -0.078911   \n",
              "1 -0.720412 -0.980596  0.055242 -0.422791 -0.043317 -0.641089 -0.078911   \n",
              "2  0.582429 -0.028609 -0.250889  0.645672 -0.043317 -0.641089 -0.078911   \n",
              "3 -0.720412  0.131617  0.788557 -0.422791 -0.043317 -0.136620 -0.078911   \n",
              "4 -0.720412  1.233833 -0.775900  1.532813 -0.043317  1.188186 -0.078911   \n",
              "\n",
              "      Shtn1  \n",
              "0 -0.195295  \n",
              "1 -0.195295  \n",
              "2 -0.195295  \n",
              "3 -0.195295  \n",
              "4 -0.195295  \n",
              "\n",
              "[5 rows x 2002 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-53accaa7-6c9c-48cf-80c4-b6464b43fe06\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Variable</th>\n",
              "      <th>cell.type</th>\n",
              "      <th>Sox17</th>\n",
              "      <th>Prex2</th>\n",
              "      <th>Terf1</th>\n",
              "      <th>Mcm3</th>\n",
              "      <th>Khdrbs2</th>\n",
              "      <th>Prim2</th>\n",
              "      <th>Bag2</th>\n",
              "      <th>Ptpn18</th>\n",
              "      <th>...</th>\n",
              "      <th>Nolc1</th>\n",
              "      <th>Nfkb2</th>\n",
              "      <th>Gsto1</th>\n",
              "      <th>Add3</th>\n",
              "      <th>Mxi1</th>\n",
              "      <th>Dusp5</th>\n",
              "      <th>Gm16299</th>\n",
              "      <th>Pdcd4</th>\n",
              "      <th>Plekhs1</th>\n",
              "      <th>Shtn1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAACCCAGTTCCACAA</td>\n",
              "      <td>cycling_capillary</td>\n",
              "      <td>-1.393788</td>\n",
              "      <td>-0.400318</td>\n",
              "      <td>1.358433</td>\n",
              "      <td>-0.494224</td>\n",
              "      <td>-0.084449</td>\n",
              "      <td>0.199815</td>\n",
              "      <td>-0.490080</td>\n",
              "      <td>-0.298271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.447022</td>\n",
              "      <td>0.738963</td>\n",
              "      <td>1.262297</td>\n",
              "      <td>-0.410792</td>\n",
              "      <td>1.378466</td>\n",
              "      <td>-0.422791</td>\n",
              "      <td>-0.043317</td>\n",
              "      <td>-0.170391</td>\n",
              "      <td>-0.078911</td>\n",
              "      <td>-0.195295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAACCCATCACGAACT</td>\n",
              "      <td>artery_1</td>\n",
              "      <td>-0.582664</td>\n",
              "      <td>-1.409803</td>\n",
              "      <td>0.177138</td>\n",
              "      <td>1.502572</td>\n",
              "      <td>-0.084449</td>\n",
              "      <td>0.554011</td>\n",
              "      <td>1.077822</td>\n",
              "      <td>1.039509</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125040</td>\n",
              "      <td>-0.603773</td>\n",
              "      <td>-0.720412</td>\n",
              "      <td>-0.980596</td>\n",
              "      <td>0.055242</td>\n",
              "      <td>-0.422791</td>\n",
              "      <td>-0.043317</td>\n",
              "      <td>-0.641089</td>\n",
              "      <td>-0.078911</td>\n",
              "      <td>-0.195295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AAACGAAAGCCGAATG</td>\n",
              "      <td>cycling_capillary</td>\n",
              "      <td>-0.146528</td>\n",
              "      <td>-0.251669</td>\n",
              "      <td>-0.801586</td>\n",
              "      <td>-0.310414</td>\n",
              "      <td>-0.084449</td>\n",
              "      <td>1.765205</td>\n",
              "      <td>-0.490080</td>\n",
              "      <td>-0.186529</td>\n",
              "      <td>...</td>\n",
              "      <td>0.318736</td>\n",
              "      <td>-0.603773</td>\n",
              "      <td>0.582429</td>\n",
              "      <td>-0.028609</td>\n",
              "      <td>-0.250889</td>\n",
              "      <td>0.645672</td>\n",
              "      <td>-0.043317</td>\n",
              "      <td>-0.641089</td>\n",
              "      <td>-0.078911</td>\n",
              "      <td>-0.195295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AAACGAAGTTCAAAGA</td>\n",
              "      <td>cycling_capillary</td>\n",
              "      <td>1.102187</td>\n",
              "      <td>-0.634825</td>\n",
              "      <td>1.040665</td>\n",
              "      <td>-0.772438</td>\n",
              "      <td>-0.084449</td>\n",
              "      <td>2.116643</td>\n",
              "      <td>0.681951</td>\n",
              "      <td>-0.487508</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.638577</td>\n",
              "      <td>-0.603773</td>\n",
              "      <td>-0.720412</td>\n",
              "      <td>0.131617</td>\n",
              "      <td>0.788557</td>\n",
              "      <td>-0.422791</td>\n",
              "      <td>-0.043317</td>\n",
              "      <td>-0.136620</td>\n",
              "      <td>-0.078911</td>\n",
              "      <td>-0.195295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAACGAATCCGACGGT</td>\n",
              "      <td>artery_2</td>\n",
              "      <td>0.863034</td>\n",
              "      <td>0.133187</td>\n",
              "      <td>0.329966</td>\n",
              "      <td>-0.557825</td>\n",
              "      <td>-0.084449</td>\n",
              "      <td>-0.616780</td>\n",
              "      <td>-0.490080</td>\n",
              "      <td>1.091608</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.194605</td>\n",
              "      <td>-0.603773</td>\n",
              "      <td>-0.720412</td>\n",
              "      <td>1.233833</td>\n",
              "      <td>-0.775900</td>\n",
              "      <td>1.532813</td>\n",
              "      <td>-0.043317</td>\n",
              "      <td>1.188186</td>\n",
              "      <td>-0.078911</td>\n",
              "      <td>-0.195295</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2002 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-53accaa7-6c9c-48cf-80c4-b6464b43fe06')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-53accaa7-6c9c-48cf-80c4-b6464b43fe06 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-53accaa7-6c9c-48cf-80c4-b6464b43fe06');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA Questions"
      ],
      "metadata": {
        "id": "m_GBx8jl8rD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. How many variations are there?\n",
        "\n",
        "df_new[\"Variable\"].nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiT2O6_O7u2q",
        "outputId": "0760edb8-6006-488c-bf9a-2099f4976a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1981"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. What is the highest sequence variation ?"
      ],
      "metadata": {
        "id": "o3xfgnvA7vtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new[\"Variable\"].value_counts(sort=True, ascending=False).head() # check if all unique"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnWUcfuL7vrI",
        "outputId": "b7f2e9d4-babe-42e0-a5a5-3691b44b0136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AAACCCAGTTCCACAA    1\n",
              "GTCTCACTCTGGTGCG    1\n",
              "GTCTAGACATCGGAAG    1\n",
              "GTCTAGACATACAGGG    1\n",
              "GTCTACCCAGGTCAAG    1\n",
              "Name: Variable, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.1\n",
        "len(df_new[\"Variable\"]) # by checking the length we see they are all unique"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DCBrV9A7voy",
        "outputId": "9ff3d209-9890-4e1d-becd-e34680544bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1981"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. What is highest correlation \n",
        "def get_redundant_pairs(df):\n",
        "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
        "    pairs_to_drop = set()\n",
        "    cols = df.columns\n",
        "    for i in range(0, df.shape[1]):\n",
        "        for j in range(0, i+1):\n",
        "            pairs_to_drop.add((cols[i], cols[j]))\n",
        "    return pairs_to_drop\n",
        "\n",
        "def get_top_abs_correlations(df, n=5):\n",
        "    df = df.drop([\"Variable\"], axis=1)\n",
        "    au_corr = df.corr().abs().unstack()\n",
        "    labels_to_drop = get_redundant_pairs(df)\n",
        "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
        "    return au_corr[0:n]\n",
        "\n",
        "print(\"Top Absolute Correlations\")\n",
        "print(get_top_abs_correlations(df_new, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1tjVFFf7vmJ",
        "outputId": "9adcba91-1766-4e3f-f163-b8e88fc99f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Absolute Correlations\n",
            "Hbb-bt  Hba-a1    0.895236\n",
            "Cdca8   Birc5     0.894852\n",
            "Nusap1  Prc1      0.890660\n",
            "Hbb-bt  Hba-a2    0.890610\n",
            "Hbb-bs  Hba-a1    0.883445\n",
            "Cenpf   Ube2c     0.883412\n",
            "Ccna2   Cdca8     0.883004\n",
            "Cenpa   Ccnb2     0.880318\n",
            "Hbb-bt  Hbb-bs    0.878251\n",
            "Ube2c   Prc1      0.878023\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. What is the most commom cell type?\n",
        "\n",
        "df_new[\"cell.type\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2Y-juzh8vgP",
        "outputId": "bfcaac52-83c0-4ad4-e981-bbf6f03560c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "cycling_capillary    1108\n",
              "artery_1              433\n",
              "capillary             201\n",
              "artery_2              185\n",
              "vein                   54\n",
              "Name: cell.type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA Summary:\n",
        "\n",
        "Looks like there are all unique gene sequences; However we can observe that some gene expression is highly correleted with others. We would expect this in some degreee based on trajectory expression of the cell's differentiation patterns. Also we see the problem will be somewhat imbalanced for our multi-classification. "
      ],
      "metadata": {
        "id": "Cx0AJlaj708T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering\n",
        "\n",
        "Plans: Polynomial Crossing or Reduction techniques "
      ],
      "metadata": {
        "id": "Kq2mWBc27AAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Test Split\n",
        " \n",
        "Split will be 90/10"
      ],
      "metadata": {
        "id": "8W72Jk-c7SuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_data = df_new\n",
        "\n",
        "X_train, X_test = train_test_split(X_data, test_size=0.1, random_state=4533) # 90/10"
      ],
      "metadata": {
        "id": "JjE0udXigQ0g"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model (Network) Pipeline\n",
        "\n",
        "Score and Evaluation will be on classification results"
      ],
      "metadata": {
        "id": "lpN2bh0o7qp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.layers import Lambda, Input, Dense\n",
        "from keras import regularizers as regularizers\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import os \n",
        "\n",
        "# input dims\n",
        "input_dim = X_train.shape[1]\n",
        "encoding_dim = 2000\n",
        "\n",
        "# model\n",
        "# 2k -> 10 -> 2k #\n",
        "input_layer = Input(shape=(input_dim, )) \n",
        "\n",
        "### Encoder\n",
        "encoder = Dense(encoding_dim, activation=\"tanh\")(input_layer)\n",
        "encoder = Dense(int(1500), activation=\"tanh\")(encoder)\n",
        "encoder = Dense(int(1000), activation=\"tanh\")(encoder)\n",
        "encoder = Dense(int(500), activation=\"tanh\")(encoder)\n",
        "encoder = Dense(int(250), activation=\"tanh\")(encoder)\n",
        "encoder = Dense(int(125), activation=\"tanh\")(encoder)\n",
        "encoder = Dense(int(75), activation=\"tanh\")(encoder)\n",
        "encoder_50 = Dense(int(50), activation=\"tanh\")(encoder)\n",
        "encoder_25 = Dense(int(25), activation=\"tanh\")(encoder_50)\n",
        "encoder_f = Dense(int(10), activation=\"tanh\")(encoder_25)\n",
        "\n",
        "### Decoder\n",
        "decoder = Dense(int(10), activation='tanh')(encoder_f)\n",
        "decoder = Dense(input_dim, activation='tanh')(decoder)\n",
        "\n",
        "### AE Model\n",
        "autoencoder = Model(inputs=input_layer, outputs = decoder)\n",
        "\n",
        "### Encoder for only encoding the Dims\n",
        "encoder = Model(inputs = input_layer, outputs = encoder_f)"
      ],
      "metadata": {
        "id": "sTaWNLIT5JSc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPQhukQLg0MP",
        "outputId": "c2018e7d-a2f9-461e-9326-a29a3907d1af"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 2000)]            0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 2000)              4002000   \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1500)              3001500   \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1000)              1501000   \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 500)               500500    \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 250)               125250    \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 125)               31375     \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 75)                9450      \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 50)                3800      \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 25)                1275      \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 10)                260       \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 10)                110       \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 2000)              22000     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,198,520\n",
            "Trainable params: 9,198,520\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epoch = 150\n",
        "batch_size = 32\n",
        "\n",
        "X_train.drop([[\"Variable\", \"cell.type\"]], axis=1, inplace=True)\n",
        "#X = np.asarray(X_train).astype(np.float32) #tensor format\n",
        "\n",
        "# Compile the model\n",
        "autoencoder.compile(optimizer='adam', \n",
        "                    loss='mean_squared_error', \n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "history = autoencoder.fit(X_train, X_train,\n",
        "                    epochs=nb_epoch,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True,\n",
        "                    #validation_data=(X_test, X_test),\n",
        "                    validation_split = 0.1,\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzjsbvvQr6Vz",
        "outputId": "50d6095c-1b75-4858-bfe7-8ee6781dfbf4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "51/51 [==============================] - 4s 13ms/step - loss: 0.8338 - accuracy: 0.0000e+00 - val_loss: 0.8012 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7997 - accuracy: 0.0000e+00 - val_loss: 0.7884 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7884 - accuracy: 0.0000e+00 - val_loss: 0.7794 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7802 - accuracy: 0.0000e+00 - val_loss: 0.7729 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7717 - accuracy: 0.0000e+00 - val_loss: 0.7641 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7623 - accuracy: 0.0000e+00 - val_loss: 0.7573 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7564 - accuracy: 0.0000e+00 - val_loss: 0.7534 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7532 - accuracy: 0.0000e+00 - val_loss: 0.7519 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7516 - accuracy: 0.0000e+00 - val_loss: 0.7506 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7502 - accuracy: 0.0000e+00 - val_loss: 0.7499 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7489 - accuracy: 0.0000e+00 - val_loss: 0.7487 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7474 - accuracy: 0.0000e+00 - val_loss: 0.7477 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7467 - accuracy: 0.0000e+00 - val_loss: 0.7474 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7459 - accuracy: 0.0000e+00 - val_loss: 0.7467 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7450 - accuracy: 0.0000e+00 - val_loss: 0.7459 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7444 - accuracy: 0.0000e+00 - val_loss: 0.7454 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7438 - accuracy: 0.0000e+00 - val_loss: 0.7453 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7426 - accuracy: 0.0000e+00 - val_loss: 0.7437 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7418 - accuracy: 0.0000e+00 - val_loss: 0.7434 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7411 - accuracy: 0.0000e+00 - val_loss: 0.7430 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7408 - accuracy: 0.0000e+00 - val_loss: 0.7419 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7394 - accuracy: 0.0000e+00 - val_loss: 0.7434 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7401 - accuracy: 0.0000e+00 - val_loss: 0.7413 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7382 - accuracy: 0.0000e+00 - val_loss: 0.7415 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7376 - accuracy: 0.0000e+00 - val_loss: 0.7417 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7372 - accuracy: 0.0000e+00 - val_loss: 0.7409 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7361 - accuracy: 0.0000e+00 - val_loss: 0.7394 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7355 - accuracy: 0.0000e+00 - val_loss: 0.7396 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7349 - accuracy: 0.0000e+00 - val_loss: 0.7390 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7345 - accuracy: 0.0000e+00 - val_loss: 0.7392 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7338 - accuracy: 0.0000e+00 - val_loss: 0.7394 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7327 - accuracy: 0.0000e+00 - val_loss: 0.7383 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7326 - accuracy: 0.0000e+00 - val_loss: 0.7385 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7324 - accuracy: 0.0000e+00 - val_loss: 0.7386 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7316 - accuracy: 0.0000e+00 - val_loss: 0.7376 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7312 - accuracy: 0.0000e+00 - val_loss: 0.7387 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7315 - accuracy: 0.0000e+00 - val_loss: 0.7384 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7314 - accuracy: 0.0000e+00 - val_loss: 0.7382 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7303 - accuracy: 0.0000e+00 - val_loss: 0.7373 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7300 - accuracy: 0.0000e+00 - val_loss: 0.7379 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7299 - accuracy: 0.0000e+00 - val_loss: 0.7368 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7291 - accuracy: 0.0000e+00 - val_loss: 0.7371 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7291 - accuracy: 0.0000e+00 - val_loss: 0.7372 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7287 - accuracy: 0.0000e+00 - val_loss: 0.7367 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7287 - accuracy: 0.0000e+00 - val_loss: 0.7360 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7279 - accuracy: 0.0000e+00 - val_loss: 0.7357 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7274 - accuracy: 0.0000e+00 - val_loss: 0.7355 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7271 - accuracy: 0.0000e+00 - val_loss: 0.7354 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7267 - accuracy: 0.0000e+00 - val_loss: 0.7364 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7264 - accuracy: 0.0000e+00 - val_loss: 0.7360 - val_accuracy: 0.0000e+00\n",
            "Epoch 51/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7262 - accuracy: 0.0000e+00 - val_loss: 0.7351 - val_accuracy: 0.0000e+00\n",
            "Epoch 52/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7264 - accuracy: 0.0000e+00 - val_loss: 0.7355 - val_accuracy: 0.0000e+00\n",
            "Epoch 53/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7258 - accuracy: 0.0000e+00 - val_loss: 0.7359 - val_accuracy: 0.0000e+00\n",
            "Epoch 54/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7258 - accuracy: 0.0000e+00 - val_loss: 0.7364 - val_accuracy: 0.0000e+00\n",
            "Epoch 55/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7253 - accuracy: 0.0000e+00 - val_loss: 0.7355 - val_accuracy: 0.0000e+00\n",
            "Epoch 56/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7252 - accuracy: 0.0000e+00 - val_loss: 0.7351 - val_accuracy: 0.0000e+00\n",
            "Epoch 57/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7249 - accuracy: 0.0000e+00 - val_loss: 0.7351 - val_accuracy: 0.0000e+00\n",
            "Epoch 58/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7250 - accuracy: 0.0000e+00 - val_loss: 0.7347 - val_accuracy: 0.0000e+00\n",
            "Epoch 59/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7245 - accuracy: 0.0000e+00 - val_loss: 0.7352 - val_accuracy: 0.0000e+00\n",
            "Epoch 60/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7243 - accuracy: 0.0000e+00 - val_loss: 0.7345 - val_accuracy: 0.0000e+00\n",
            "Epoch 61/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7236 - accuracy: 0.0000e+00 - val_loss: 0.7341 - val_accuracy: 0.0000e+00\n",
            "Epoch 62/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7233 - accuracy: 0.0000e+00 - val_loss: 0.7346 - val_accuracy: 0.0000e+00\n",
            "Epoch 63/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7235 - accuracy: 0.0000e+00 - val_loss: 0.7351 - val_accuracy: 0.0000e+00\n",
            "Epoch 64/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7234 - accuracy: 0.0000e+00 - val_loss: 0.7352 - val_accuracy: 0.0000e+00\n",
            "Epoch 65/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7235 - accuracy: 0.0000e+00 - val_loss: 0.7343 - val_accuracy: 0.0000e+00\n",
            "Epoch 66/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7229 - accuracy: 0.0000e+00 - val_loss: 0.7356 - val_accuracy: 0.0000e+00\n",
            "Epoch 67/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7225 - accuracy: 0.0000e+00 - val_loss: 0.7353 - val_accuracy: 0.0000e+00\n",
            "Epoch 68/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7223 - accuracy: 0.0000e+00 - val_loss: 0.7352 - val_accuracy: 0.0000e+00\n",
            "Epoch 69/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7221 - accuracy: 0.0000e+00 - val_loss: 0.7352 - val_accuracy: 0.0000e+00\n",
            "Epoch 70/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7217 - accuracy: 0.0000e+00 - val_loss: 0.7341 - val_accuracy: 0.0000e+00\n",
            "Epoch 71/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7217 - accuracy: 0.0000e+00 - val_loss: 0.7349 - val_accuracy: 0.0000e+00\n",
            "Epoch 72/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7218 - accuracy: 0.0000e+00 - val_loss: 0.7349 - val_accuracy: 0.0000e+00\n",
            "Epoch 73/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7219 - accuracy: 0.0000e+00 - val_loss: 0.7346 - val_accuracy: 0.0000e+00\n",
            "Epoch 74/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7213 - accuracy: 0.0000e+00 - val_loss: 0.7357 - val_accuracy: 0.0000e+00\n",
            "Epoch 75/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7220 - accuracy: 0.0000e+00 - val_loss: 0.7349 - val_accuracy: 0.0000e+00\n",
            "Epoch 76/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7217 - accuracy: 0.0000e+00 - val_loss: 0.7341 - val_accuracy: 0.0000e+00\n",
            "Epoch 77/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7211 - accuracy: 0.0000e+00 - val_loss: 0.7340 - val_accuracy: 0.0000e+00\n",
            "Epoch 78/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7210 - accuracy: 0.0000e+00 - val_loss: 0.7336 - val_accuracy: 0.0000e+00\n",
            "Epoch 79/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7203 - accuracy: 0.0000e+00 - val_loss: 0.7339 - val_accuracy: 0.0000e+00\n",
            "Epoch 80/150\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.7203 - accuracy: 0.0000e+00 - val_loss: 0.7329 - val_accuracy: 0.0000e+00\n",
            "Epoch 81/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7206 - accuracy: 0.0000e+00 - val_loss: 0.7337 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/150\n",
            "51/51 [==============================] - 1s 14ms/step - loss: 0.7204 - accuracy: 0.0000e+00 - val_loss: 0.7346 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7209 - accuracy: 0.0000e+00 - val_loss: 0.7342 - val_accuracy: 0.0000e+00\n",
            "Epoch 84/150\n",
            "51/51 [==============================] - 1s 15ms/step - loss: 0.7205 - accuracy: 0.0000e+00 - val_loss: 0.7346 - val_accuracy: 0.0000e+00\n",
            "Epoch 85/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7201 - accuracy: 0.0000e+00 - val_loss: 0.7324 - val_accuracy: 0.0000e+00\n",
            "Epoch 86/150\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.7195 - accuracy: 0.0000e+00 - val_loss: 0.7326 - val_accuracy: 0.0000e+00\n",
            "Epoch 87/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7190 - accuracy: 0.0000e+00 - val_loss: 0.7328 - val_accuracy: 0.0000e+00\n",
            "Epoch 88/150\n",
            "51/51 [==============================] - 1s 14ms/step - loss: 0.7191 - accuracy: 0.0000e+00 - val_loss: 0.7325 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7190 - accuracy: 0.0000e+00 - val_loss: 0.7324 - val_accuracy: 0.0000e+00\n",
            "Epoch 90/150\n",
            "51/51 [==============================] - 1s 11ms/step - loss: 0.7189 - accuracy: 0.0000e+00 - val_loss: 0.7326 - val_accuracy: 0.0000e+00\n",
            "Epoch 91/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7185 - accuracy: 0.0000e+00 - val_loss: 0.7326 - val_accuracy: 0.0000e+00\n",
            "Epoch 92/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7181 - accuracy: 0.0000e+00 - val_loss: 0.7327 - val_accuracy: 0.0000e+00\n",
            "Epoch 93/150\n",
            "51/51 [==============================] - 1s 11ms/step - loss: 0.7179 - accuracy: 0.0000e+00 - val_loss: 0.7322 - val_accuracy: 0.0000e+00\n",
            "Epoch 94/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7179 - accuracy: 0.0000e+00 - val_loss: 0.7320 - val_accuracy: 0.0000e+00\n",
            "Epoch 95/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7182 - accuracy: 0.0000e+00 - val_loss: 0.7332 - val_accuracy: 0.0000e+00\n",
            "Epoch 96/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7181 - accuracy: 0.0000e+00 - val_loss: 0.7329 - val_accuracy: 0.0000e+00\n",
            "Epoch 97/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7180 - accuracy: 0.0000e+00 - val_loss: 0.7336 - val_accuracy: 0.0000e+00\n",
            "Epoch 98/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7177 - accuracy: 0.0000e+00 - val_loss: 0.7320 - val_accuracy: 0.0000e+00\n",
            "Epoch 99/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7181 - accuracy: 0.0000e+00 - val_loss: 0.7326 - val_accuracy: 0.0000e+00\n",
            "Epoch 100/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7187 - accuracy: 0.0000e+00 - val_loss: 0.7340 - val_accuracy: 0.0000e+00\n",
            "Epoch 101/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7186 - accuracy: 0.0000e+00 - val_loss: 0.7331 - val_accuracy: 0.0000e+00\n",
            "Epoch 102/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7184 - accuracy: 0.0000e+00 - val_loss: 0.7328 - val_accuracy: 0.0000e+00\n",
            "Epoch 103/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7181 - accuracy: 0.0000e+00 - val_loss: 0.7324 - val_accuracy: 0.0000e+00\n",
            "Epoch 104/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7178 - accuracy: 0.0000e+00 - val_loss: 0.7321 - val_accuracy: 0.0000e+00\n",
            "Epoch 105/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7176 - accuracy: 0.0000e+00 - val_loss: 0.7321 - val_accuracy: 0.0000e+00\n",
            "Epoch 106/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7177 - accuracy: 0.0000e+00 - val_loss: 0.7323 - val_accuracy: 0.0000e+00\n",
            "Epoch 107/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7171 - accuracy: 0.0000e+00 - val_loss: 0.7313 - val_accuracy: 0.0000e+00\n",
            "Epoch 108/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7168 - accuracy: 0.0000e+00 - val_loss: 0.7308 - val_accuracy: 0.0000e+00\n",
            "Epoch 109/150\n",
            "51/51 [==============================] - 1s 11ms/step - loss: 0.7164 - accuracy: 0.0000e+00 - val_loss: 0.7310 - val_accuracy: 0.0000e+00\n",
            "Epoch 110/150\n",
            "51/51 [==============================] - 0s 6ms/step - loss: 0.7163 - accuracy: 0.0000e+00 - val_loss: 0.7317 - val_accuracy: 0.0000e+00\n",
            "Epoch 111/150\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.7163 - accuracy: 0.0000e+00 - val_loss: 0.7314 - val_accuracy: 0.0000e+00\n",
            "Epoch 112/150\n",
            "51/51 [==============================] - 1s 13ms/step - loss: 0.7162 - accuracy: 0.0000e+00 - val_loss: 0.7319 - val_accuracy: 0.0000e+00\n",
            "Epoch 113/150\n",
            "51/51 [==============================] - 1s 13ms/step - loss: 0.7161 - accuracy: 0.0000e+00 - val_loss: 0.7323 - val_accuracy: 0.0000e+00\n",
            "Epoch 114/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7161 - accuracy: 0.0000e+00 - val_loss: 0.7321 - val_accuracy: 0.0000e+00\n",
            "Epoch 115/150\n",
            "51/51 [==============================] - 1s 14ms/step - loss: 0.7157 - accuracy: 0.0000e+00 - val_loss: 0.7311 - val_accuracy: 0.0000e+00\n",
            "Epoch 116/150\n",
            "51/51 [==============================] - 1s 13ms/step - loss: 0.7161 - accuracy: 0.0000e+00 - val_loss: 0.7307 - val_accuracy: 0.0000e+00\n",
            "Epoch 117/150\n",
            "51/51 [==============================] - 1s 14ms/step - loss: 0.7160 - accuracy: 0.0000e+00 - val_loss: 0.7321 - val_accuracy: 0.0000e+00\n",
            "Epoch 118/150\n",
            "51/51 [==============================] - 1s 14ms/step - loss: 0.7163 - accuracy: 0.0000e+00 - val_loss: 0.7313 - val_accuracy: 0.0000e+00\n",
            "Epoch 119/150\n",
            "51/51 [==============================] - 1s 14ms/step - loss: 0.7160 - accuracy: 0.0000e+00 - val_loss: 0.7321 - val_accuracy: 0.0000e+00\n",
            "Epoch 120/150\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.7161 - accuracy: 0.0000e+00 - val_loss: 0.7318 - val_accuracy: 0.0000e+00\n",
            "Epoch 121/150\n",
            "51/51 [==============================] - 1s 13ms/step - loss: 0.7158 - accuracy: 0.0000e+00 - val_loss: 0.7313 - val_accuracy: 0.0000e+00\n",
            "Epoch 122/150\n",
            "51/51 [==============================] - 1s 11ms/step - loss: 0.7159 - accuracy: 0.0000e+00 - val_loss: 0.7319 - val_accuracy: 0.0000e+00\n",
            "Epoch 123/150\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.7160 - accuracy: 0.0000e+00 - val_loss: 0.7311 - val_accuracy: 0.0000e+00\n",
            "Epoch 124/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7158 - accuracy: 0.0000e+00 - val_loss: 0.7311 - val_accuracy: 0.0000e+00\n",
            "Epoch 125/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7158 - accuracy: 0.0000e+00 - val_loss: 0.7328 - val_accuracy: 0.0000e+00\n",
            "Epoch 126/150\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.7158 - accuracy: 0.0000e+00 - val_loss: 0.7321 - val_accuracy: 0.0000e+00\n",
            "Epoch 127/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7158 - accuracy: 0.0000e+00 - val_loss: 0.7318 - val_accuracy: 0.0000e+00\n",
            "Epoch 128/150\n",
            "51/51 [==============================] - 0s 7ms/step - loss: 0.7157 - accuracy: 0.0000e+00 - val_loss: 0.7312 - val_accuracy: 0.0000e+00\n",
            "Epoch 129/150\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.7158 - accuracy: 0.0000e+00 - val_loss: 0.7318 - val_accuracy: 0.0000e+00\n",
            "Epoch 130/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7163 - accuracy: 0.0000e+00 - val_loss: 0.7311 - val_accuracy: 0.0000e+00\n",
            "Epoch 131/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7159 - accuracy: 0.0000e+00 - val_loss: 0.7310 - val_accuracy: 0.0000e+00\n",
            "Epoch 132/150\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.7154 - accuracy: 0.0000e+00 - val_loss: 0.7304 - val_accuracy: 0.0000e+00\n",
            "Epoch 133/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7149 - accuracy: 0.0000e+00 - val_loss: 0.7303 - val_accuracy: 0.0000e+00\n",
            "Epoch 134/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7154 - accuracy: 0.0000e+00 - val_loss: 0.7302 - val_accuracy: 0.0000e+00\n",
            "Epoch 135/150\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.7147 - accuracy: 0.0000e+00 - val_loss: 0.7306 - val_accuracy: 0.0000e+00\n",
            "Epoch 136/150\n",
            "51/51 [==============================] - 1s 15ms/step - loss: 0.7145 - accuracy: 0.0000e+00 - val_loss: 0.7313 - val_accuracy: 0.0000e+00\n",
            "Epoch 137/150\n",
            "51/51 [==============================] - 1s 15ms/step - loss: 0.7145 - accuracy: 0.0000e+00 - val_loss: 0.7303 - val_accuracy: 0.0000e+00\n",
            "Epoch 138/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7146 - accuracy: 0.0000e+00 - val_loss: 0.7305 - val_accuracy: 0.0000e+00\n",
            "Epoch 139/150\n",
            "51/51 [==============================] - 1s 13ms/step - loss: 0.7145 - accuracy: 0.0000e+00 - val_loss: 0.7308 - val_accuracy: 0.0000e+00\n",
            "Epoch 140/150\n",
            "51/51 [==============================] - 1s 13ms/step - loss: 0.7144 - accuracy: 0.0000e+00 - val_loss: 0.7298 - val_accuracy: 0.0000e+00\n",
            "Epoch 141/150\n",
            "51/51 [==============================] - 0s 9ms/step - loss: 0.7142 - accuracy: 0.0000e+00 - val_loss: 0.7303 - val_accuracy: 0.0000e+00\n",
            "Epoch 142/150\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.7141 - accuracy: 0.0000e+00 - val_loss: 0.7302 - val_accuracy: 0.0000e+00\n",
            "Epoch 143/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7141 - accuracy: 0.0000e+00 - val_loss: 0.7309 - val_accuracy: 0.0000e+00\n",
            "Epoch 144/150\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.7139 - accuracy: 0.0000e+00 - val_loss: 0.7302 - val_accuracy: 0.0000e+00\n",
            "Epoch 145/150\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.7140 - accuracy: 0.0000e+00 - val_loss: 0.7299 - val_accuracy: 0.0000e+00\n",
            "Epoch 146/150\n",
            "51/51 [==============================] - 1s 10ms/step - loss: 0.7137 - accuracy: 0.0000e+00 - val_loss: 0.7300 - val_accuracy: 0.0000e+00\n",
            "Epoch 147/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7141 - accuracy: 0.0000e+00 - val_loss: 0.7314 - val_accuracy: 0.0000e+00\n",
            "Epoch 148/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7143 - accuracy: 0.0000e+00 - val_loss: 0.7299 - val_accuracy: 0.0000e+00\n",
            "Epoch 149/150\n",
            "51/51 [==============================] - 1s 14ms/step - loss: 0.7145 - accuracy: 0.0000e+00 - val_loss: 0.7308 - val_accuracy: 0.0000e+00\n",
            "Epoch 150/150\n",
            "51/51 [==============================] - 1s 12ms/step - loss: 0.7141 - accuracy: 0.0000e+00 - val_loss: 0.7303 - val_accuracy: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(autoencoder.history.history['loss'])\n",
        "plt.plot(autoencoder.history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "zijQ7PWWsjju",
        "outputId": "cfcbf905-914d-4424-b642-83ce03fc6a2c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fef84203690>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Encoded Embeddings \n",
        "X_train_enc = encoder.predict(<input>)"
      ],
      "metadata": {
        "id": "yjasSuIytYeP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}